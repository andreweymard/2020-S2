{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 1: DECLARE THE MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 2: Data import and preprocess\n",
    "#Run this but dont worry if it does not make any sense Jump to SECTION 3 that is related to your HD task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "DataSet = \"Week_5_NSL-KDD-Dataset/training_attack_types.txt\"\n",
    "DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "header_names = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate','rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack_type', 'success_pred']\n",
    "\n",
    "col_names = np.array(header_names)\n",
    "\n",
    "nominal_idx = [1, 2, 3]\n",
    "binary_idx = [6, 11, 13, 14, 20, 21]\n",
    "numeric_idx = list(set(range(41)).difference(nominal_idx).difference(binary_idx)) #doesn't contain numbers in above 2 variables\n",
    "\n",
    "nominal_cols = col_names[nominal_idx].tolist() #gives col names in header names according to number held in idx var\n",
    "binary_cols = col_names[binary_idx].tolist()   #basically differentating according to type\n",
    "numeric_cols = col_names[numeric_idx].tolist()\n",
    "\n",
    "category = defaultdict(list)\n",
    "category['benign'].append('normal')\n",
    "\n",
    "with open(DataSet, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        attack, cat = line.strip().split(' ')\n",
    "        category[cat].append(attack)\n",
    "\n",
    "attack_mapping = dict((v,k) for k in category for v in category[k])\n",
    "\n",
    "attack_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Processing Training Data\n",
    "train_file = \"Week_5_NSL-KDD-Dataset/KDDTrain+.txt\"\n",
    "\n",
    "train_df = pd.read_csv(train_file, names=header_names)\n",
    "train_df['attack_category'] = train_df['attack_type'].map(lambda x: attack_mapping[x])\n",
    "train_df.drop(['success_pred'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Processing test Data\n",
    "test_file = \"Week_5_NSL-KDD-Dataset/KDDTest+.txt\"\n",
    "\n",
    "test_df = pd.read_csv(test_file, names=header_names)\n",
    "test_df['attack_category'] = test_df['attack_type'].map(lambda x: attack_mapping[x]) #adds column with type defined in mapping\n",
    "test_df.drop(['success_pred'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_attack_types = train_df['attack_type'].value_counts()\n",
    "train_attack_cats = train_df['attack_category'].value_counts()\n",
    "\n",
    "test_attack_types = test_df['attack_type'].value_counts()\n",
    "test_attack_cats = test_df['attack_category'].value_counts()\n",
    "\n",
    "train_attack_types.plot(kind='barh', figsize=(20,10), fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_attack_cats.plot(kind='barh', figsize=(20,10), fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_attack_types.plot(kind='barh', figsize=(20,15), fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_attack_cats.plot(kind='barh', figsize=(20,10), fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_df[binary_cols].describe().transpose()\n",
    "train_df.groupby(['su_attempted']).size()\n",
    "train_df['su_attempted'].replace(2, 0, inplace=True)\n",
    "test_df['su_attempted'].replace(2, 0, inplace=True)\n",
    "train_df.groupby(['su_attempted']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_df.groupby(['num_outbound_cmds']).size()\n",
    "train_df.drop('num_outbound_cmds', axis=1, inplace=True)\n",
    "test_df.drop('num_outbound_cmds', axis=1, inplace=True)\n",
    "numeric_cols.remove('num_outbound_cmds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Data Preparation\n",
    "train_Y = train_df['attack_category']\n",
    "train_x_raw = train_df.drop(['attack_category', 'attack_type'], axis=1)\n",
    "test_Y = test_df['attack_category']\n",
    "test_x_raw = test_df.drop(['attack_category', 'attack_type'], axis=1)\n",
    "\n",
    "combined_df_raw = pd.concat([train_x_raw, test_x_raw])\n",
    "combined_df = pd.get_dummies(combined_df_raw, columns=nominal_cols, drop_first=True)\n",
    "\n",
    "train_x = combined_df[:len(train_x_raw)]\n",
    "test_x = combined_df[len(train_x_raw):]\n",
    "\n",
    "# Store dummy variable feature names\n",
    "dummy_variables = list(set(train_x)-set(combined_df_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_x.describe()\n",
    "train_x['duration'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Experimenting with StandardScaler on the single 'duration' feature\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "durations = train_x['duration'].values.reshape(-1, 1)\n",
    "standard_scaler = StandardScaler().fit(durations)\n",
    "scaled_durations = standard_scaler.transform(durations)\n",
    "pd.Series(scaled_durations.flatten()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Experimenting with MinMaxScaler on the single 'duration' feature\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler().fit(durations)\n",
    "min_max_scaled_durations = min_max_scaler.transform(durations)\n",
    "pd.Series(min_max_scaled_durations.flatten()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Experimenting with RobustScaler on the single 'duration' feature\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "min_max_scaler = RobustScaler().fit(durations)\n",
    "robust_scaled_durations = min_max_scaler.transform(durations)\n",
    "pd.Series(robust_scaled_durations.flatten()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Experimenting with MaxAbsScaler on the single 'duration' feature\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "max_Abs_scaler = MaxAbsScaler().fit(durations)\n",
    "robust_scaled_durations = max_Abs_scaler.transform(durations)\n",
    "pd.Series(robust_scaled_durations.flatten()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Let's proceed with StandardScaler- Apply to all the numeric columns\n",
    "\n",
    "standard_scaler = StandardScaler().fit(train_x[numeric_cols])\n",
    "train_x[numeric_cols] = standard_scaler.transform(train_x[numeric_cols])\n",
    "test_x[numeric_cols] = standard_scaler.transform(test_x[numeric_cols])\n",
    "train_x.describe()\n",
    "\n",
    "train_Y_bin = train_Y.apply(lambda x: 0 if x is 'benign' else 1)\n",
    "test_Y_bin = test_Y.apply(lambda x: 0 if x is 'benign' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 3: Multi class classification\n",
    "#This is the section where you have to add other algorithms, tune algorithms and visualize to compare and analyze algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 5-class classification version\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "start = time.time()\n",
    "tree = DecisionTreeClassifier(random_state=17)\n",
    "tree.fit(train_x, train_Y)\n",
    "pred_y = tree.predict(test_x)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def output():\n",
    "    names = ('benign', 'dos', 'probe', 'r2l', 'u2r')\n",
    "    conf_matrix = metrics.confusion_matrix(test_Y, pred_y)\n",
    "    TP = np.diag(conf_matrix)\n",
    "    FP = conf_matrix.sum(axis=0) - TP\n",
    "    FN = conf_matrix.sum(axis=1) - TP\n",
    "    TN = conf_matrix.sum() - (FP + FN + TP)\n",
    "    FPR = (FP/(FP+TN))\n",
    "    \n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(\"\\nClassification report:\\n\", metrics.classification_report(test_Y, pred_y, digits=5))\n",
    "    for i in range(5):\n",
    "        print(\"FPR of \", names[i], \" is: {:.5f}\" .format(FPR[i]))\n",
    "    print(\"\\nAccuracy Score: {:.5f}\" .format(metrics.accuracy_score(test_Y, pred_y)))\n",
    "    print(\"Zero one loss: {:.5f}\" .format(metrics.zero_one_loss(test_Y, pred_y)))\n",
    "    print(\"Time taken: {:.5f}\" .format(end - start), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Classifier results are shown below.\\n\")\n",
    "output()\n",
    "print(\"ROC area: \", metrics.roc_auc_score(test_Y, tree.predict_proba(test_x), multi_class=\"ovr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "start = time.time()\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights=\"distance\")\n",
    "knn.fit(train_x, train_Y)\n",
    "pred_y = knn.predict(test_x)\n",
    "end = time.time()\n",
    "print(\"K-Nearest Neighbors classifier results are shown below.\")\n",
    "output()\n",
    "print(\"\\nROC area: \", metrics.roc_auc_score(test_Y, knn.predict_proba(test_x), multi_class=\"ovr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "start = time.time()\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_x, train_Y)\n",
    "pred_y = gnb.predict(test_x)\n",
    "end = time.time()\n",
    "print(\"Gaussian Naive Bayes classifier results are shown below.\")\n",
    "output()\n",
    "print(\"\\nROC area: \", metrics.roc_auc_score(test_Y, gnb.predict_proba(test_x), multi_class=\"ovr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "start = time.time()\n",
    "clf = LogisticRegression(C=0.75, random_state=17)\n",
    "clf.fit(train_x, train_Y)\n",
    "pred_y = clf.predict(test_x)\n",
    "end = time.time()\n",
    "print(\"Logistic Regression classifier results are shown below.\")\n",
    "output()\n",
    "print(\"\\nROC area: \", metrics.roc_auc_score(test_Y, clf.predict_proba(test_x), multi_class=\"ovr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "start = time.time()\n",
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(train_x, train_Y)\n",
    "pred_y = svc.predict(test_x)\n",
    "end = time.time()\n",
    "print(\"Support Vector Machine classifier results are shown below.\")\n",
    "output()\n",
    "# print(\"\\nROC area: \", metrics.roc_auc_score(test_Y, svc.predict_proba(test_x), multi_class=\"ovr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "start = time.time()\n",
    "svc = SVC(kernel='rbf', C=0.75)\n",
    "svc.fit(train_x, train_Y)\n",
    "pred_y = svc.predict(test_x)\n",
    "end = time.time()\n",
    "print(\"Support Vector Machine classifier results are shown below.\")\n",
    "output()\n",
    "# print(\"\\nROC area: \", metrics.roc_auc_score(test_Y, svc.predict_proba(test_x), multi_class=\"ovr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# start = time.time()\n",
    "# rfc = RandomForestClassifier(random_state=17)\n",
    "# rfc.fit(train_x, train_Y)\n",
    "# pred_y = rfc.predict(test_x)\n",
    "# end = time.time()\n",
    "# output()\n",
    "# print(\"\\nROC area: \", metrics.roc_auc_score(test_Y, rfc.predict_proba(test_x), multi_class=\"ovr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "start = time.time()\n",
    "neural = MLPClassifier(random_state=17)\n",
    "neural.fit(train_x, train_Y)\n",
    "pred_y = neural.predict(test_x)\n",
    "end = time.time()\n",
    "print(\"Multi-Layer Perceptron classifier results are shown below.\")\n",
    "output()\n",
    "print(\"\\nROC area: \", metrics.roc_auc_score(test_Y, neural.predict_proba(test_x), multi_class=\"ovr\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
